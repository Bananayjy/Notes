## 2025-02-27

### 一、乱码问题

第三方推送的json数据

```json
{
    "typeCode": "33002",
    "deviceTypeCode": "34002",
    "beginDate": "2025-02-27",
    "orderId": "C2025022744264",
    "jobNumber": "49517",
    "purchaseOrderType": 1,
    "projectId": 1881254305551478786,
    "reason": "pcr产物回收，用于研究基因功能的检测",
    "purchaseFiles": [
        {
            "fileBase64": "iV",
            "fileName": "7b1487db-6ef7-4764-9e58-b83f14a3c20a.png",
            "fileFix": "png"
        }
    ],
    "materialDetails": [
        {
            "goodsName": "PCR产物纯化试剂盒",
            "goodsAmount": 3,
            "unitPrice": 418,
            "agent": "南京奥瑞德生物技术有限公司",
            "specifications": " 200次 ",
            "expectedPrice": 1254,
            "detailOrderId": "DC202502274426401"
        }
    ],
    "projectEstimateReimbursements": [
        {
            "fundType": 1871381139853275137,
            "cardType": 17,
            "estimatedAmount": 1254.00
        }
    ]
}
```

接受的时候是通过一个Object对象：xxxDto进行接受的，SpringMVC在将json转为xxxDto的时候发生了异常

```
JSON parse error: Unexpected character ('è' (code 232)): was expecting comma to separate Object entries; nested exception is com.fasterxml.jackson.databind.JsonMappingException: Unexpected character ('è' (code 232)): was expecting comma to separate Object entries
 at [Source: (org.springframework.util.StreamUtils$NonClosingInputStream); line: 1, column: 6923973] (through reference chain: com.hscloud.hs.invoice.model.dto.reimbursement.ruiJing.RJSubmitDTO["materialDetails"]->java.util.ArrayList[4])
```

最后问题是materialDetails中的specifications的前后有\t，导致解码的时候发生异常，解析成了è。



在 Linux 系统中，字符集（Character Set）和语言环境（Locale）是密切相关的。字符集决定了系统如何编码和解码文本数据。Linux 系统的字符集通常由语言环境（Locale）决定。可以通过以下命令查看当前的语言环境设置：

```
locale
```

查看结果：

```
LANG=en_US.UTF-8
LC_CTYPE="en_US.UTF-8"
LC_NUMERIC="en_US.UTF-8"
LC_TIME="en_US.UTF-8"
LC_COLLATE="en_US.UTF-8"
LC_MONETARY="en_US.UTF-8"
LC_MESSAGES="en_US.UTF-8"
LC_PAPER="en_US.UTF-8"
LC_NAME="en_US.UTF-8"
LC_ADDRESS="en_US.UTF-8"
LC_TELEPHONE="en_US.UTF-8"
LC_MEASUREMENT="en_US.UTF-8"
LC_IDENTIFICATION="en_US.UTF-8"
LC_ALL=
```

